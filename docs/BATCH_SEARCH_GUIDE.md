# æ‰¹é‡æ£€ç´¢æŒ‡å— (Batch Search Guide)

## æ¦‚è¿°

æ‰¹é‡æ£€ç´¢åŠŸèƒ½å…è®¸åœ¨å•ä¸ªAPIè¯·æ±‚ä¸­å¤„ç†å¤šä¸ªæŸ¥è¯¢ï¼Œç®€åŒ–å®¢æˆ·ç«¯ä»£ç å¹¶æä¾›ç»Ÿä¸€çš„æ€§èƒ½ç»Ÿè®¡ã€‚

## æ ¸å¿ƒç‰¹æ€§

### 1. æ‰¹é‡å¤„ç†
- ä¸€æ¬¡è¯·æ±‚å¤„ç†å¤šä¸ªæŸ¥è¯¢æ–‡ä»¶
- ç»Ÿä¸€çš„å‚æ•°é…ç½®
- èšåˆçš„ç»“æœè¿”å›

### 2. å¤„ç†æ¨¡å¼
- **é¡ºåºæ¨¡å¼**ï¼šæŒ‰é¡ºåºé€ä¸ªå¤„ç†æŸ¥è¯¢ï¼ˆé»˜è®¤å…³é—­å¹¶è¡Œï¼‰
- **å¹¶è¡Œæ¨¡å¼**ï¼šä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼ˆé€‚åˆI/Oå¯†é›†å‹åœºæ™¯ï¼‰

### 3. æ€§èƒ½ç»Ÿè®¡
- æ€»æŸ¥è¯¢æ•°é‡
- æˆåŠŸ/å¤±è´¥è®¡æ•°
- æ€»è€—æ—¶å’Œå¹³å‡æ¯æŸ¥è¯¢è€—æ—¶
- QPSï¼ˆæ¯ç§’æŸ¥è¯¢æ•°ï¼‰

### 4. é”™è¯¯å¤„ç†
- å•ä¸ªæŸ¥è¯¢å¤±è´¥ä¸å½±å“å…¶ä»–æŸ¥è¯¢
- è¿”å›æ¯ä¸ªæŸ¥è¯¢çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯
- èšåˆç»Ÿè®¡æ‰€æœ‰æˆåŠŸå’Œå¤±è´¥çš„æŸ¥è¯¢

## API æ¥å£

### ç«¯ç‚¹
```
POST /search/batch
```

### è¯·æ±‚å‚æ•°

```python
{
    "query_file_paths": [str],  # æŸ¥è¯¢æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¿…éœ€ï¼‰
    "k": int,                    # è¿”å›Top-Kç»“æœï¼ˆé»˜è®¤20ï¼‰
    "stage1_topn": int,          # ä¸¤é˜¶æ®µæ£€ç´¢ç¬¬ä¸€é˜¶æ®µå€™é€‰æ•°ï¼ˆé»˜è®¤100ï¼‰
    "fusion_method": str,        # èåˆæ–¹æ³•ï¼š"weighted"/"rrf"/"borda"ï¼ˆé»˜è®¤weightedï¼‰
    "alpha": float,              # ç¬¬ä¸€é˜¶æ®µæƒé‡ï¼ˆé»˜è®¤0.6ï¼‰
    "beta": float,               # ç¬¬äºŒé˜¶æ®µæƒé‡ï¼ˆé»˜è®¤0.4ï¼‰
    "filters": dict,             # å…ƒæ•°æ®è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼‰
    "explainable": bool,         # æ˜¯å¦è¿”å›å¯è§£é‡Šæ€§åˆ†æï¼ˆé»˜è®¤falseï¼‰
    "parallel": bool             # æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼ˆé»˜è®¤trueï¼‰
}
```

### å“åº”æ ¼å¼

```json
{
    "status": "success",
    "total_queries": 50,
    "successful": 50,
    "failed": 0,
    "elapsed_time": 2.430,
    "avg_time_per_query": 0.049,
    "parallel": false,
    "results": {
        "/path/to/query1.h5": {
            "status": "success",
            "results": [...],
            "explanation": {...}  // ä»…åœ¨explainable=trueæ—¶
        },
        "/path/to/query2.h5": {
            "status": "error",
            "error": "Query file not found"
        }
    }
}
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºæœ¬æ‰¹é‡æ£€ç´¢

```python
import requests

# å‡†å¤‡æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨
query_files = [
    "/path/to/queries/00000000.h5",
    "/path/to/queries/00000001.h5",
    "/path/to/queries/00000002.h5"
]

# å‘é€æ‰¹é‡æ£€ç´¢è¯·æ±‚
response = requests.post(
    "http://localhost:8000/search/batch",
    json={
        "query_file_paths": query_files,
        "k": 10,
        "parallel": False  # ä½¿ç”¨é¡ºåºæ¨¡å¼
    }
)

result = response.json()
print(f"å¤„ç†äº† {result['total_queries']} ä¸ªæŸ¥è¯¢")
print(f"æˆåŠŸ: {result['successful']}, å¤±è´¥: {result['failed']}")
print(f"å¹³å‡è€—æ—¶: {result['avg_time_per_query']:.3f}ç§’/æŸ¥è¯¢")
```

### 2. å¸¦æ··åˆæ£€ç´¢çš„æ‰¹é‡æŸ¥è¯¢

```python
# ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤
response = requests.post(
    "http://localhost:8000/search/batch",
    json={
        "query_file_paths": query_files,
        "k": 20,
        "filters": {
            "subset": "0000",      # åªåœ¨å­é›†0000ä¸­æœç´¢
            "min_seq_len": 50,     # æœ€å°åºåˆ—é•¿åº¦
            "max_seq_len": 200     # æœ€å¤§åºåˆ—é•¿åº¦
        },
        "parallel": True
    }
)
```

### 3. å¸¦å¯è§£é‡Šæ€§çš„æ‰¹é‡æŸ¥è¯¢

```python
# å¯ç”¨å¯è§£é‡Šæ€§åˆ†æ
response = requests.post(
    "http://localhost:8000/search/batch",
    json={
        "query_file_paths": query_files,
        "k": 10,
        "stage1_topn": 100,
        "explainable": True,  # è¿”å›ç›¸ä¼¼åº¦åˆ†è§£
        "parallel": False     # å¤æ‚æŸ¥è¯¢å»ºè®®ä½¿ç”¨é¡ºåºæ¨¡å¼
    },
    timeout=180  # å¢åŠ è¶…æ—¶æ—¶é—´
)

# è®¿é—®ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„è§£é‡Š
first_query = query_files[0]
explanation = result['results'][first_query]['explanation']
print(f"èåˆæ–¹æ³•: {explanation['fusion_method']}")
print(f"æœ€ä½³åŒ¹é…: {explanation['top_match']['id']}")
```

### 4. å®Œæ•´çš„æ€§èƒ½æµ‹è¯•è„šæœ¬

```python
#!/usr/bin/env python3
import requests
import time
from pathlib import Path

API_URL = "http://localhost:8000"

# æ”¶é›†æŸ¥è¯¢æ–‡ä»¶
query_files = []
data_dir = Path("/path/to/data/vec/0000")
for h5_file in sorted(data_dir.glob("*.h5"))[:100]:
    query_files.append(str(h5_file))

print(f"å‡†å¤‡æµ‹è¯• {len(query_files)} ä¸ªæŸ¥è¯¢")

# æµ‹è¯•é¡ºåºæ¨¡å¼
print("\nğŸ”„ æµ‹è¯•é¡ºåºæ¨¡å¼...")
start = time.time()
response = requests.post(
    f"{API_URL}/search/batch",
    json={
        "query_file_paths": query_files,
        "k": 10,
        "parallel": False
    },
    timeout=300
)
seq_time = time.time() - start
seq_result = response.json()

print(f"âœ… é¡ºåºæ¨¡å¼: {seq_result['successful']} æŸ¥è¯¢æˆåŠŸ")
print(f"   è€—æ—¶: {seq_time:.3f}ç§’")
print(f"   QPS: {len(query_files)/seq_time:.1f} æŸ¥è¯¢/ç§’")

# æµ‹è¯•å¹¶è¡Œæ¨¡å¼
time.sleep(2)
print("\nâš¡ æµ‹è¯•å¹¶è¡Œæ¨¡å¼...")
start = time.time()
response = requests.post(
    f"{API_URL}/search/batch",
    json={
        "query_file_paths": query_files,
        "k": 10,
        "parallel": True
    },
    timeout=300
)
par_time = time.time() - start
par_result = response.json()

print(f"âœ… å¹¶è¡Œæ¨¡å¼: {par_result['successful']} æŸ¥è¯¢æˆåŠŸ")
print(f"   è€—æ—¶: {par_time:.3f}ç§’")
print(f"   QPS: {len(query_files)/par_time:.1f} æŸ¥è¯¢/ç§’")
print(f"   åŠ é€Ÿæ¯”: {seq_time/par_time:.2f}x")
```

## æ€§èƒ½ç‰¹å¾

### å®æµ‹ç»“æœï¼ˆ500å‘é‡ç´¢å¼•ï¼‰

#### ç®€å•æŸ¥è¯¢ï¼ˆé»˜è®¤å‚æ•°ï¼‰
- **10ä¸ªæŸ¥è¯¢**
  - é¡ºåºæ¨¡å¼: 0.190ç§’ (52.6 QPS)
  - å¹¶è¡Œæ¨¡å¼: 0.192ç§’ (52.1 QPS)
  - åŠ é€Ÿæ¯”: 0.99x

- **50ä¸ªæŸ¥è¯¢**
  - é¡ºåºæ¨¡å¼: 0.659ç§’ (75.8 QPS)
  - å¹¶è¡Œæ¨¡å¼: 0.797ç§’ (62.7 QPS)
  - åŠ é€Ÿæ¯”: 0.83x

#### å¤æ‚æŸ¥è¯¢ï¼ˆä¸¤é˜¶æ®µæ£€ç´¢ + å¯è§£é‡Šæ€§ï¼‰
- **50ä¸ªæŸ¥è¯¢**
  - é¡ºåºæ¨¡å¼: 2.430ç§’ (19.6 QPS)
  - å¹¶è¡Œæ¨¡å¼: 2.729ç§’ (17.7 QPS)
  - åŠ é€Ÿæ¯”: 0.90x

### æ€§èƒ½åˆ†æ

å½“å‰å®ç°ä½¿ç”¨ `ThreadPoolExecutor`ï¼ˆå¤šçº¿ç¨‹ï¼‰ï¼Œç”±äºPythonçš„å…¨å±€è§£é‡Šå™¨é”(GIL)é™åˆ¶ï¼š

1. **CPUå¯†é›†å‹ä»»åŠ¡**ï¼šFAISSå‘é‡æœç´¢å’Œnumpyè®¡ç®—æ— æ³•çœŸæ­£å¹¶è¡ŒåŒ–
2. **çº¿ç¨‹å¼€é”€**ï¼šçº¿ç¨‹åˆ›å»ºå’Œç®¡ç†çš„å¼€é”€æŠµæ¶ˆäº†å¹¶è¡Œæ”¶ç›Š
3. **å†…å­˜è®¿é—®**ï¼šå¤§é‡å†…å­˜è®¿é—®æ“ä½œåœ¨GILä¸‹ä¸²è¡Œæ‰§è¡Œ

**ç»“è®º**ï¼šæ‰¹é‡æ£€ç´¢ä¸»è¦ä½œä¸º**ä¾¿æ·æ€§åŠŸèƒ½**ï¼Œç®€åŒ–å®¢æˆ·ç«¯ä»£ç å’Œæä¾›ç»Ÿä¸€çš„é”™è¯¯å¤„ç†ï¼Œè€Œä¸æ˜¯æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ã€‚

### ä½¿ç”¨å»ºè®®

1. **å°æ‰¹é‡æŸ¥è¯¢ï¼ˆ< 10ï¼‰**ï¼šä½¿ç”¨é¡ºåºæ¨¡å¼ï¼Œé¿å…çº¿ç¨‹å¼€é”€
2. **ä¸­ç­‰æ‰¹é‡ï¼ˆ10-50ï¼‰**ï¼šé¡ºåºæ¨¡å¼é€šå¸¸æ›´å¿«
3. **å¤§æ‰¹é‡ï¼ˆ> 50ï¼‰**ï¼šå¯å°è¯•å¹¶è¡Œæ¨¡å¼ï¼Œä½†æ•ˆæœæœ‰é™
4. **å¤æ‚æŸ¥è¯¢**ï¼šå§‹ç»ˆä½¿ç”¨é¡ºåºæ¨¡å¼ï¼ˆexplainable=trueï¼‰

## é”™è¯¯å¤„ç†

### å•ä¸ªæŸ¥è¯¢å¤±è´¥

```python
{
    "status": "success",  # æ•´ä½“çŠ¶æ€ä»ä¸ºsuccess
    "total_queries": 3,
    "successful": 2,
    "failed": 1,
    "results": {
        "query1.h5": {"status": "success", "results": [...]},
        "query2.h5": {"status": "error", "error": "File not found"},
        "query3.h5": {"status": "success", "results": [...]}
    }
}
```

### æ‰¹é‡è¯·æ±‚å¤±è´¥

```python
{
    "detail": "Batch search error: max_workers must be greater than 0"
}
```

## é«˜çº§ç”¨æ³•

### 1. åŠ¨æ€æ–‡ä»¶æ”¶é›†

```python
from pathlib import Path

# ä»ç›®å½•æ”¶é›†æ‰€æœ‰.h5æ–‡ä»¶
data_dir = Path("/path/to/data")
query_files = [str(f) for f in data_dir.rglob("*.h5")][:100]

# æŒ‰å­é›†åˆ†ç»„æ‰¹é‡æŸ¥è¯¢
subsets = {}
for f in query_files:
    subset = f.split('/')[-2]  # æå–å­é›†ç¼–å·
    if subset not in subsets:
        subsets[subset] = []
    subsets[subset].append(f)

# åˆ†æ‰¹å¤„ç†æ¯ä¸ªå­é›†
for subset, files in subsets.items():
    response = requests.post(
        f"{API_URL}/search/batch",
        json={"query_file_paths": files, "k": 10}
    )
    print(f"å­é›† {subset}: {response.json()['successful']} æˆåŠŸ")
```

### 2. ç»“æœèšåˆå’Œåˆ†æ

```python
# æ”¶é›†æ‰€æœ‰Top-1ç»“æœ
top1_results = {}
for query_path, query_result in result['results'].items():
    if query_result['status'] == 'success':
        top1 = query_result['results'][0]
        top1_results[query_path] = {
            'id': top1['id'],
            'score': top1['score']
        }

# è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
avg_score = sum(r['score'] for r in top1_results.values()) / len(top1_results)
print(f"å¹³å‡Top-1ç›¸ä¼¼åº¦: {avg_score:.4f}")
```

### 3. å¢é‡é‡è¯•å¤±è´¥çš„æŸ¥è¯¢

```python
# ç¬¬ä¸€æ¬¡æ‰¹é‡è¯·æ±‚
response = requests.post(f"{API_URL}/search/batch", json={
    "query_file_paths": all_queries,
    "k": 10
})
result = response.json()

# æ”¶é›†å¤±è´¥çš„æŸ¥è¯¢
failed_queries = [
    path for path, res in result['results'].items()
    if res['status'] == 'error'
]

if failed_queries:
    print(f"é‡è¯• {len(failed_queries)} ä¸ªå¤±è´¥çš„æŸ¥è¯¢...")
    retry_response = requests.post(f"{API_URL}/search/batch", json={
        "query_file_paths": failed_queries,
        "k": 10
    })
```

## å¯¹æ¯”å•æ¬¡æ£€ç´¢

### æ‰¹é‡æ£€ç´¢çš„ä¼˜åŠ¿
1. **ç®€åŒ–ä»£ç **ï¼šä¸€æ¬¡è¯·æ±‚å¤„ç†å¤šä¸ªæŸ¥è¯¢
2. **ç»Ÿä¸€é…ç½®**ï¼šæ‰€æœ‰æŸ¥è¯¢ä½¿ç”¨ç›¸åŒå‚æ•°
3. **ç»Ÿè®¡ä¿¡æ¯**ï¼šè‡ªåŠ¨è®¡ç®—æ€§èƒ½æŒ‡æ ‡
4. **é”™è¯¯èšåˆ**ï¼šé›†ä¸­å¤„ç†æ‰€æœ‰é”™è¯¯

### å•æ¬¡æ£€ç´¢çš„ä¼˜åŠ¿
1. **çµæ´»æ€§**ï¼šæ¯ä¸ªæŸ¥è¯¢å¯ä½¿ç”¨ä¸åŒå‚æ•°
2. **æµå¼å¤„ç†**ï¼šå¯é€ä¸ªå¤„ç†ç»“æœ
3. **æ›´ä½å»¶è¿Ÿ**ï¼šå•ä¸ªæŸ¥è¯¢ç«‹å³è¿”å›
4. **èµ„æºæ§åˆ¶**ï¼šæ›´å¥½çš„å¹¶å‘æ§åˆ¶

## æœªæ¥ä¼˜åŒ–æ–¹å‘

å¦‚éœ€è¦çœŸæ­£çš„æ€§èƒ½æå‡ï¼Œå¯è€ƒè™‘ï¼š

1. **å¤šè¿›ç¨‹å¤„ç†**ï¼šä½¿ç”¨ `ProcessPoolExecutor` ç»•è¿‡GILé™åˆ¶
2. **å¼‚æ­¥I/O**ï¼šä½¿ç”¨ `asyncio` å¤„ç†I/Oå¯†é›†å‹æ“ä½œ
3. **GPUåŠ é€Ÿ**ï¼šä½¿ç”¨FAISS GPUç‰ˆæœ¬è¿›è¡Œå¹¶è¡Œæœç´¢
4. **åˆ†å¸ƒå¼ç´¢å¼•**ï¼šå°†ç´¢å¼•åˆ†ç‰‡åˆ°å¤šä¸ªèŠ‚ç‚¹
5. **æŸ¥è¯¢ç¼“å­˜**ï¼šç¼“å­˜å¸¸è§æŸ¥è¯¢çš„ç»“æœ

## æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šå¹¶è¡Œæ¨¡å¼æ¯”é¡ºåºæ¨¡å¼æ…¢
**åŸå› **ï¼šçº¿ç¨‹å¼€é”€ + Python GILé™åˆ¶  
**è§£å†³**ï¼šä½¿ç”¨é¡ºåºæ¨¡å¼ï¼ˆ`parallel: false`ï¼‰

### é—®é¢˜2ï¼šè¶…æ—¶é”™è¯¯
**åŸå› **ï¼šæŸ¥è¯¢æ•°é‡å¤ªå¤šæˆ–æŸ¥è¯¢è¿‡äºå¤æ‚  
**è§£å†³**ï¼š
- å‡å°‘æ‰¹é‡å¤§å°
- å¢åŠ å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´
- å…³é—­å¯è§£é‡Šæ€§åˆ†æï¼ˆ`explainable: false`ï¼‰

### é—®é¢˜3ï¼šéƒ¨åˆ†æŸ¥è¯¢å¤±è´¥
**åŸå› **ï¼šæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ–‡ä»¶æ ¼å¼é”™è¯¯  
**è§£å†³**ï¼š
- æ£€æŸ¥ `results` ä¸­çš„é”™è¯¯ä¿¡æ¯
- éªŒè¯æ‰€æœ‰æ–‡ä»¶è·¯å¾„
- é‡è¯•å¤±è´¥çš„æŸ¥è¯¢

## æ€»ç»“

æ‰¹é‡æ£€ç´¢åŠŸèƒ½æä¾›äº†ä¾¿æ·çš„æ‰¹é‡æŸ¥è¯¢æ¥å£ï¼Œé€‚åˆï¼š
- æ‰¹é‡è¯„ä¼°å’Œæµ‹è¯•
- ç®€åŒ–å®¢æˆ·ç«¯ä»£ç 
- ç»Ÿä¸€çš„æ€§èƒ½ç»Ÿè®¡
- é›†ä¸­å¼é”™è¯¯å¤„ç†

å¯¹äºæ€§èƒ½æ•æ„Ÿçš„åº”ç”¨ï¼Œå»ºè®®æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„å¤„ç†æ¨¡å¼ï¼Œæˆ–è€ƒè™‘ä½¿ç”¨å¤šè¿›ç¨‹/åˆ†å¸ƒå¼æ–¹æ¡ˆã€‚
